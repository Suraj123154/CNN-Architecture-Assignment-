{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e9ad2a2"
      },
      "source": [
        "### Question 1: What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "\n",
        "**Answer:**\n",
        "*   **Filters (Kernels):** Small matrices of numbers that slide across the input data (e.g., image) to detect specific features (edges, textures, patterns). Each filter specializes in detecting a different feature.\n",
        "*   **Feature Maps (Activation Maps):** The output of applying a filter to an input. Each value in a feature map indicates the presence and strength of the detected feature at different locations in the input. Multiple feature maps are generated by different filters to form a comprehensive representation of the input's features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2caefbe0"
      },
      "source": [
        "### Question 2: Explain the concepts of padding and stride in CNNs (Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "**Answer:**\n",
        "*   **Padding:** Adding extra rows and columns (usually zeros) around the border of the input image. It's used to preserve spatial information at the edges and to control the output size, preventing reduction in dimensions. 'Same' padding maintains output size, while 'valid' (no) padding reduces it.\n",
        "*   **Stride:** The number of pixels a filter shifts over the input matrix at each step. A larger stride reduces the spatial dimensions of the output feature map, effectively downsampling the input.\n",
        "\n",
        "**Effect on Output Dimensions:**\n",
        "Output Size = `[(W - F + 2P) / S] + 1`\n",
        "Where:\n",
        "*   `W` = Input size (width or height)\n",
        "*   `F` = Filter size\n",
        "*   `P` = Padding amount\n",
        "*   `S` = Stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79029757"
      },
      "source": [
        "### Question 3: Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        "**Answer:**\n",
        "*   **Receptive Field:** The region in the input image that a particular neuron in a feature map is 'looking at' or influenced by. As layers deepen in a CNN, the receptive field of neurons in subsequent layers grows, meaning they incorporate information from a larger area of the original input.\n",
        "*   **Importance for Deep Architectures:** It allows deeper layers to capture more abstract and complex features by integrating information from a wider spatial context. A larger receptive field enables the model to understand global patterns and relationships in the input, which is crucial for tasks requiring high-level understanding like object recognition and scene interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25ea9726"
      },
      "source": [
        "### Question 4: Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "**Answer:**\n",
        "*   **Filter Size:** A larger filter size directly increases the number of parameters within that specific filter. For example, a 5x5 filter has 25 parameters, while a 3x3 filter has 9. The total number of parameters also depends on the number of input channels and output filters.\n",
        "*   **Stride:** Stride does **not** directly influence the number of parameters within the filters themselves. However, a larger stride reduces the spatial dimensions of the output feature maps. This reduction in feature map size means that subsequent layers will process smaller inputs, potentially leading to a smaller number of parameters in those subsequent layers if they are fully connected or have fewer filters due to the reduced input dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01fc78f4"
      },
      "source": [
        "### Question 5: Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "**Answer:**\n",
        "| Feature           | LeNet-5                          | AlexNet                           | VGG-16/19                               |\n",
        "| :---------------- | :------------------------------- | :-------------------------------- | :-------------------------------------- |\n",
        "| **Depth**         | Shallow (5 conv, 2 FC layers)    | Medium (5 conv, 3 FC layers)      | Very Deep (16 or 19 conv, 3 FC layers)  |\n",
        "| **Filter Sizes**  | 5x5 (larger initial filters)     | Primarily 11x11, 5x5, 3x3 (larger initial filters) | Exclusively 3x3 (small, uniform filters) |\n",
        "| **Performance**   | Good for small, simple images (e.g., MNIST) | Significant breakthrough on ImageNet, outperformed LeNet | State-of-the-art on ImageNet, very strong performance, deeper but computationally intensive |\n",
        "| **Key Innovation**| First successful CNN, local receptive fields, shared weights, pooling | ReLU activation, GPU training, overlapping pooling, dropout, data augmentation | Uniform 3x3 filters, increased depth, simplicity of architecture |\n",
        "| **Parameters**    | ~60k                             | ~60M                              | ~138M (VGG-16), ~144M (VGG-19)          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bea9b21"
      },
      "source": [
        "\n",
        "#Question 6 (Keras MNIST CNN): Using Keras, build and train a simple CNN model on the MNIST dataset from scratch. This includes code for model creation, compilation, training, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc87b8b1",
        "outputId": "0f6d0109-005c-4bbb-ae2a-818b68811075"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(f\"Original x_train shape: {x_train.shape}\")\n",
        "print(f\"Original y_train shape: {y_train.shape}\")\n",
        "print(f\"Original x_test shape: {x_test.shape}\")\n",
        "print(f\"Original y_test shape: {y_test.shape}\")\n",
        "\n",
        "# 2. Preprocess the data\n",
        "# a. Reshape the input images to include a channel dimension (e.g., (28, 28, 1))\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
        "\n",
        "# b. Normalize the pixel values to a range of 0 to 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# c. Convert the labels to one-hot encoded vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "print(f\"Reshaped and normalized x_train shape: {x_train.shape}\")\n",
        "print(f\"One-hot encoded y_train shape: {y_train.shape}\")\n",
        "print(f\"Reshaped and normalized x_test shape: {x_test.shape}\")\n",
        "print(f\"One-hot encoded y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original x_train shape: (60000, 28, 28)\n",
            "Original y_train shape: (60000,)\n",
            "Original x_test shape: (10000, 28, 28)\n",
            "Original y_test shape: (10000,)\n",
            "Reshaped and normalized x_train shape: (60000, 28, 28, 1)\n",
            "One-hot encoded y_train shape: (60000, 10)\n",
            "Reshaped and normalized x_test shape: (10000, 28, 28, 1)\n",
            "One-hot encoded y_test shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c01d2a9"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is preprocessed, the next step is to define the CNN model architecture as specified in the instructions. This involves stacking several convolutional, pooling, flatten, and dense layers using `tf.keras.Sequential`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "fc2af6e3",
        "outputId": "d936f171-1e59-4852-da42-ded8a21285f8"
      },
      "source": [
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "print(\"CNN model architecture defined successfully.\")\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model architecture defined successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_29 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_30 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │       \u001b[38;5;34m102,464\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">102,464</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,930\u001b[0m (476.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,930</span> (476.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121,930\u001b[0m (476.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,930</span> (476.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4f6746e",
        "outputId": "9b309459-8daa-4fd1-d924-7a43cb7aeced"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"CNN model compiled successfully.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77b68b44"
      },
      "source": [
        "**Reasoning**:\n",
        "With the model compiled, the next step is to train it using the preprocessed training data and labels, as specified in the instructions. I will use 5 epochs and a batch size of 64.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f858558",
        "outputId": "6d9bc3ed-0067-4423-8ce6-48add880abfe"
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.8749 - loss: 0.4231 - val_accuracy: 0.9792 - val_loss: 0.0687\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.0568 - val_accuracy: 0.9860 - val_loss: 0.0446\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9877 - loss: 0.0396 - val_accuracy: 0.9898 - val_loss: 0.0339\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9911 - loss: 0.0275 - val_accuracy: 0.9873 - val_loss: 0.0438\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0226 - val_accuracy: 0.9920 - val_loss: 0.0305\n",
            "CNN model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75437cbb"
      },
      "source": [
        "**Reasoning**:\n",
        "The model has been trained, and now I need to evaluate its performance on the test data as specified in the instructions. This will involve using the `model.evaluate()` method with the preprocessed test data and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a531b44",
        "outputId": "ba0c2fc3-4dac-4120-b2fc-0d77bbc27194"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"\\nTest accuracy: {test_acc}\")\n",
        "print(f\"Test loss: {test_loss}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - 5ms/step - accuracy: 0.9920 - loss: 0.0260\n",
            "\n",
            "Test accuracy: 0.9919999837875366\n",
            "Test loss: 0.02601625584065914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f444a430"
      },
      "source": [
        "\n",
        "#Question 7 : Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show the preprocessing steps and the model architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "f4d55766",
        "outputId": "1c454338-1a33-42e3-e270-126eea2935c3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the CIFAR-10 dataset\n",
        "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = tf.keras.datasets.cifar10.load_data()\n",
        "print(f\"Original x_train_cifar shape: {x_train_cifar.shape}\")\n",
        "print(f\"Original y_train_cifar shape: {y_train_cifar.shape}\")\n",
        "print(f\"Original x_test_cifar shape: {x_test_cifar.shape}\")\n",
        "print(f\"Original y_test_cifar shape: {y_test_cifar.shape}\")\n",
        "\n",
        "# 2. Normalize the pixel values\n",
        "x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
        "x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
        "\n",
        "# 3. Convert labels to one-hot encoded vectors\n",
        "y_train_cifar = tf.keras.utils.to_categorical(y_train_cifar, 10)\n",
        "y_test_cifar = tf.keras.utils.to_categorical(y_test_cifar, 10)\n",
        "\n",
        "print(f\"Normalized x_train_cifar shape: {x_train_cifar.shape}\")\n",
        "print(f\"One-hot encoded y_train_cifar shape: {y_train_cifar.shape}\")\n",
        "print(f\"Normalized x_test_cifar shape: {x_test_cifar.shape}\")\n",
        "print(f\"One-hot encoded y_test_cifar shape: {y_test_cifar.shape}\")\n",
        "\n",
        "# 4. Define a simple CNN model architecture\n",
        "cifar_model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax') # 10 classes for CIFAR-10\n",
        "])\n",
        "\n",
        "# 5. Print the model summary\n",
        "print(\"\\nCIFAR-10 CNN model architecture defined successfully.\")\n",
        "cifar_model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original x_train_cifar shape: (50000, 32, 32, 3)\n",
            "Original y_train_cifar shape: (50000, 1)\n",
            "Original x_test_cifar shape: (10000, 32, 32, 3)\n",
            "Original y_test_cifar shape: (10000, 1)\n",
            "Normalized x_train_cifar shape: (50000, 32, 32, 3)\n",
            "One-hot encoded y_train_cifar shape: (50000, 10)\n",
            "Normalized x_test_cifar shape: (10000, 32, 32, 3)\n",
            "One-hot encoded y_test_cifar shape: (10000, 10)\n",
            "\n",
            "CIFAR-10 CNN model architecture defined successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_31 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_32 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_11 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d82814f7",
        "outputId": "71a57f6b-2f61-4511-9af1-4e374ef49d1d"
      },
      "source": [
        "cifar_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"CIFAR-10 CNN model compiled successfully.\")\n",
        "\n",
        "history_cifar = cifar_model.fit(x_train_cifar, y_train_cifar, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "print(\"CIFAR-10 CNN model training completed.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 CNN model compiled successfully.\n",
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.3289 - loss: 1.8187 - val_accuracy: 0.5012 - val_loss: 1.4012\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5503 - loss: 1.2535 - val_accuracy: 0.6064 - val_loss: 1.1190\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6125 - loss: 1.0973 - val_accuracy: 0.6342 - val_loss: 1.0184\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6477 - loss: 0.9983 - val_accuracy: 0.6672 - val_loss: 0.9518\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6775 - loss: 0.9157 - val_accuracy: 0.6684 - val_loss: 0.9431\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7005 - loss: 0.8452 - val_accuracy: 0.6778 - val_loss: 0.9158\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7263 - loss: 0.7804 - val_accuracy: 0.7058 - val_loss: 0.8622\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7305 - loss: 0.7557 - val_accuracy: 0.7114 - val_loss: 0.8338\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7560 - loss: 0.6950 - val_accuracy: 0.7168 - val_loss: 0.8483\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7739 - loss: 0.6490 - val_accuracy: 0.7054 - val_loss: 0.8679\n",
            "CIFAR-10 CNN model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed2256bc",
        "outputId": "ac7b8640-3148-45e2-d982-46accaadd514"
      },
      "source": [
        "test_loss_cifar, test_acc_cifar = cifar_model.evaluate(x_test_cifar, y_test_cifar, verbose=2)\n",
        "\n",
        "print(f\"\\nCIFAR-10 Test accuracy: {test_acc_cifar}\")\n",
        "print(f\"CIFAR-10 Test loss: {test_loss_cifar}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - 5ms/step - accuracy: 0.6969 - loss: 0.9014\n",
            "\n",
            "CIFAR-10 Test accuracy: 0.6969000101089478\n",
            "CIFAR-10 Test loss: 0.9014440178871155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc6329fb"
      },
      "source": [
        "\n",
        "#Question 8 :Using PyTorch, write a script to define and train a CNN on the MNIST dataset. This will include model definition, data loaders, a training loop, and accuracy evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b053e602",
        "outputId": "78a703cb-475b-4100-dd32-fefe15e93e98"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"PyTorch libraries imported successfully.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6da006b",
        "outputId": "d53d35af-13cc-4331-c98c-ca9bff8f6457"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Calculate the input size for the first fully connected layer\n",
        "        # MNIST images are 28x28. After two Conv2D(padding=1) and two MaxPool2D(kernel_size=2, stride=2):\n",
        "        # 1st Conv: (28 - 3 + 2*1)/1 + 1 = 28\n",
        "        # 1st Pool: 28 / 2 = 14\n",
        "        # 2nd Conv: (14 - 3 + 2*1)/1 + 1 = 14\n",
        "        # 2nd Pool: 14 / 2 = 7\n",
        "        # Output channels from conv2 is 64, so flattened size is 64 * 7 * 7\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "print(\"CNN model defined successfully:\")\n",
        "print(model)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model defined successfully:\n",
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu1): ReLU()\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu2): ReLU()\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99f89831",
        "outputId": "c4ece12f-34be-4822-afff-f9a449e87563"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "print(\"MNIST dataset loaded and transformed successfully.\")\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST dataset loaded and transformed successfully.\n",
            "Number of training samples: 60000\n",
            "Number of testing samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c29e659",
        "outputId": "c905eb1c-8798-4e6f-f635-566b3eff294f"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"DataLoaders created with batch size: {batch_size}\")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of testing batches: {len(test_loader)}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders created with batch size: 64\n",
            "Number of training batches: 938\n",
            "Number of testing batches: 157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "059315d5",
        "outputId": "064dfe96-f248-411e-e238-14e6b19574c8"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"Loss function (CrossEntropyLoss) and optimizer (Adam) defined successfully.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function (CrossEntropyLoss) and optimizer (Adam) defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f69d373",
        "outputId": "5c6b3ff7-ff9d-4b50-dc35-9dc85145b33f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the CNN model architecture (from cell c6da006b)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Calculate the input size for the first fully connected layer\n",
        "        # MNIST images are 28x28. After two Conv2D(padding=1) and two MaxPool2D(kernel_size=2, stride=2):\n",
        "        # 1st Conv: (28 - 3 + 2*1)/1 + 1 = 28\n",
        "        # 1st Pool: 28 / 2 = 14\n",
        "        # 2nd Conv: (14 - 3 + 2*1)/1 + 1 = 14\n",
        "        # 2nd Pool: 14 / 2 = 7\n",
        "        # Output channels from conv2 is 64, so flattened size is 64 * 7 * 7\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model (from cell c6da006b)\n",
        "model = CNN()\n",
        "\n",
        "# Define transformations (from cell 99f89831)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset (from cell 99f89831)\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader (from cell 6c29e659)\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define Loss function and Optimizer (from cell 059315d5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Training model on {device}\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "print(\"PyTorch model training completed.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 57.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.63MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.8MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.97MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on cpu\n",
            "Epoch [1/5], Loss: 0.1237\n",
            "Epoch [2/5], Loss: 0.0396\n",
            "Epoch [3/5], Loss: 0.0280\n",
            "Epoch [4/5], Loss: 0.0186\n",
            "Epoch [5/5], Loss: 0.0151\n",
            "PyTorch model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17a921f1"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution of the training loop only showed output for the first epoch, suggesting it might have been interrupted or the output truncated. To ensure the model completes training for all 5 specified epochs as required, I will re-execute the full training loop including all necessary definitions to restore the environment state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a73bb67",
        "outputId": "21fc525c-e4bb-4f26-9b20-248d64c6607e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the CNN model architecture (from cell c6da006b)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Calculate the input size for the first fully connected layer\n",
        "        # MNIST images are 28x28. After two Conv2D(padding=1) and two MaxPool2D(kernel_size=2, stride=2):\n",
        "        # 1st Conv: (28 - 3 + 2*1)/1 + 1 = 28\n",
        "        # 1st Pool: 28 / 2 = 14\n",
        "        # 2nd Conv: (14 - 3 + 2*1)/1 + 1 = 14\n",
        "        # 2nd Pool: 14 / 2 = 7\n",
        "        # Output channels from conv2 is 64, so flattened size is 64 * 7 * 7\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model (from cell c6da006b)\n",
        "model = CNN()\n",
        "\n",
        "# Define transformations (from cell 99f89831)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset (from cell 99f89831)\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader (from cell 6c29e659)\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define Loss function and Optimizer (from cell 059315d5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Training model on {device}\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "print(\"PyTorch model training completed.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on cpu\n",
            "Epoch [1/5], Loss: 0.1315\n",
            "Epoch [2/5], Loss: 0.0420\n",
            "Epoch [3/5], Loss: 0.0281\n",
            "Epoch [4/5], Loss: 0.0208\n",
            "Epoch [5/5], Loss: 0.0168\n",
            "PyTorch model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79d5df2e"
      },
      "source": [
        "## Evaluate PyTorch Model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained PyTorch CNN model on the MNIST test dataset to calculate its final accuracy and loss.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c88352a"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the trained PyTorch CNN model, I need to define the evaluation logic, including loading the test dataset, setting up the test data loader, and performing a forward pass without gradient calculations to compute the test loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9a7ce3",
        "outputId": "e5d80ce5-c213-4d6a-be49-37c40295b15e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the CNN model architecture (from cell c6da006b)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model (from cell c6da006b)\n",
        "model = CNN()\n",
        "\n",
        "# Define transformations (from cell 99f89831)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset (from cell 99f89831)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader (from cell 6c29e659)\n",
        "batch_size = 64\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Loss function (from cell 059315d5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# Disable gradient calculations during evaluation\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0) # Multiply by batch size to get total loss for batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 2.2990\n",
            "Test Accuracy: 10.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f99212"
      },
      "source": [
        "\n",
        "#Question 9 :LGiven a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f06f29ad",
        "outputId": "f76f1655-558f-42ad-9ecd-bab1ee2e09b2"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the base directory for the custom dataset\n",
        "base_dir = 'custom_dataset'\n",
        "\n",
        "# Define class names\n",
        "class_names = ['cat', 'dog'] # Using more descriptive class names for simulation\n",
        "\n",
        "# Define the number of dummy images per class for train and validation\n",
        "num_train_images_per_class = 10\n",
        "num_val_images_per_class = 5\n",
        "\n",
        "# Create the base directory if it doesn't exist\n",
        "if not os.path.exists(base_dir):\n",
        "    os.makedirs(base_dir)\n",
        "    print(f\"Created directory: {base_dir}\")\n",
        "\n",
        "# Create 'train' and 'validation' directories\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "for directory in [train_dir, validation_dir]:\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"Created directory: {directory}\")\n",
        "\n",
        "# Create class subdirectories and populate with dummy image files\n",
        "for class_name in class_names:\n",
        "    # Training data\n",
        "    train_class_dir = os.path.join(train_dir, class_name)\n",
        "    if not os.path.exists(train_class_dir):\n",
        "        os.makedirs(train_class_dir)\n",
        "        print(f\"Created directory: {train_class_dir}\")\n",
        "    for i in range(num_train_images_per_class):\n",
        "        dummy_file_path = os.path.join(train_class_dir, f'{class_name}_{i+1:03d}.jpg')\n",
        "        with open(dummy_file_path, 'w') as f:\n",
        "            f.write('') # Create an empty file as a dummy image\n",
        "\n",
        "    # Validation data\n",
        "    val_class_dir = os.path.join(validation_dir, class_name)\n",
        "    if not os.path.exists(val_class_dir):\n",
        "        os.makedirs(val_class_dir)\n",
        "        print(f\"Created directory: {val_class_dir}\")\n",
        "    for i in range(num_val_images_per_class):\n",
        "        dummy_file_path = os.path.join(val_class_dir, f'{class_name}_{i+1:03d}.jpg')\n",
        "        with open(dummy_file_path, 'w') as f:\n",
        "            f.write('') # Create an empty file as a dummy image\n",
        "\n",
        "print(\"Custom dataset directory structure simulated successfully.\")\n",
        "print(f\"Train images per class: {num_train_images_per_class}\")\n",
        "print(f\"Validation images per class: {num_val_images_per_class}\")\n",
        "print(f\"Total classes: {len(class_names)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory: custom_dataset\n",
            "Created directory: custom_dataset/train\n",
            "Created directory: custom_dataset/validation\n",
            "Created directory: custom_dataset/train/cat\n",
            "Created directory: custom_dataset/validation/cat\n",
            "Created directory: custom_dataset/train/dog\n",
            "Created directory: custom_dataset/validation/dog\n",
            "Custom dataset directory structure simulated successfully.\n",
            "Train images per class: 10\n",
            "Validation images per class: 5\n",
            "Total classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc10cd3f",
        "outputId": "c0bbc9d3-8b0d-422b-894e-06ce50942986"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Image dimensions\n",
        "IMG_WIDTH = 150\n",
        "IMG_HEIGHT = 150\n",
        "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3) # RGB images\n",
        "\n",
        "# Instantiate ImageDataGenerator for training with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, # Normalize pixel values to [0, 1]\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Instantiate ImageDataGenerator for validation (only rescaling)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "print(\"ImageDataGenerators for training and validation instantiated successfully.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageDataGenerators for training and validation instantiated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fb6ec74",
        "outputId": "d241ceaf-9e54-47aa-c96e-032ae3c67f9f"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Create train_generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create validation_generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "print(\"Train and validation data generators created successfully.\")\n",
        "print(f\"Found {train_generator.samples} training images belonging to {train_generator.num_classes} classes.\")\n",
        "print(f\"Found {validation_generator.samples} validation images belonging to {validation_generator.num_classes} classes.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Train and validation data generators created successfully.\n",
            "Found 20 training images belonging to 2 classes.\n",
            "Found 10 validation images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "58431a52",
        "outputId": "bfa92b25-ba23-48ec-8e7d-2b91effabd57"
      },
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=IMG_SHAPE),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(len(class_names), activation='softmax') # Number of classes\n",
        "])\n",
        "\n",
        "print(\"CNN model architecture defined successfully.\")\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model architecture defined successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m18,940,416\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m1,026\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,940,416</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,034,690\u001b[0m (72.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,690</span> (72.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,034,690\u001b[0m (72.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,690</span> (72.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36edf026"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the CNN model architecture is defined, the next step is to compile it using an appropriate optimizer, loss function, and metrics, as specified in the instructions. I will use 'adam' optimizer, 'categorical_crossentropy' loss, and 'accuracy' metric.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2447bcd",
        "outputId": "10cfee65-33bb-4fea-acf4-271b184b1e4e"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"CNN model compiled successfully.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95a158e2"
      },
      "source": [
        "**Reasoning**:\n",
        "The `UnidentifiedImageError` occurred because the dummy image files created in a previous step were empty and not valid image formats for `ImageDataGenerator`. To fix this, I will regenerate the training code block, but critically, I will first ensure that valid, small image files are created for the simulated dataset using the `PIL` library, and then proceed with the `ImageDataGenerator` setup, model definition, compilation, and training, all within this single code block to ensure the environment is correctly configured.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8856070e",
        "outputId": "0a98e452-ffd6-4cb2-cdea-aa7a26d11c1d"
      },
      "source": [
        "import os\n",
        "from PIL import Image # Import Pillow for image creation\n",
        "import shutil # Import shutil for directory cleanup\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the base directory for the custom dataset\n",
        "base_dir = 'custom_dataset'\n",
        "class_names = ['cat', 'dog']\n",
        "num_train_images_per_class = 10\n",
        "num_val_images_per_class = 5\n",
        "IMG_WIDTH = 150\n",
        "IMG_HEIGHT = 150\n",
        "\n",
        "# --- Corrected dummy image creation logic ---\n",
        "# Ensure directories are clean before creating new images to avoid conflicts from previous runs\n",
        "if os.path.exists(base_dir):\n",
        "    shutil.rmtree(base_dir) # Remove existing faulty directory\n",
        "\n",
        "os.makedirs(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "for directory in [train_dir, validation_dir]:\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Create class subdirectories and populate with actual dummy image files\n",
        "print(\"Creating dummy image files...\")\n",
        "for class_name in class_names:\n",
        "    for prefix, num_images, target_dir in [('train', num_train_images_per_class, train_dir),\n",
        "                                            ('val', num_val_images_per_class, validation_dir)]:\n",
        "        class_dir = os.path.join(target_dir, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        for i in range(num_images):\n",
        "            dummy_file_path = os.path.join(class_dir, f'{class_name}_{i+1:03d}.jpg')\n",
        "            # Create a small, valid black image using Pillow\n",
        "            img = Image.new('RGB', (IMG_WIDTH, IMG_HEIGHT), color = 'black')\n",
        "            img.save(dummy_file_path)\n",
        "print(\"Dummy image files created successfully.\")\n",
        "\n",
        "# --- ImageDataGenerator setup ---\n",
        "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3) # RGB images\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, # Normalize pixel values to [0, 1]\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "print(\"Train and validation data generators created successfully.\")\n",
        "\n",
        "# --- CNN Model Definition ---\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=IMG_SHAPE),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(len(class_names), activation='softmax') # Number of classes\n",
        "])\n",
        "print(\"CNN model architecture defined successfully.\")\n",
        "\n",
        "# --- Model Compilation ---\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"CNN model compiled successfully.\")\n",
        "\n",
        "# --- Model Training ---\n",
        "epochs = 10\n",
        "\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating dummy image files...\n",
            "Dummy image files created successfully.\n",
            "Found 20 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Train and validation data generators created successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model architecture defined successfully.\n",
            "CNN model compiled successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "CNN model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6a8591f"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to evaluate the trained PyTorch CNN model on the MNIST test dataset. This involves reloading the necessary components, setting the model to evaluation mode, iterating through the test data, calculating predictions, and then computing the final test loss and accuracy. I will ensure all required imports and model/data setup are present in this cell to avoid any state issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62a5c95d",
        "outputId": "c69c73b8-f896-484c-9a73-ee62b265bd23"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the CNN model architecture (re-definition for completeness)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST test dataset\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 64\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load the state_dict from the previously trained model\n",
        "# Assuming `model` object retains its trained weights from cell 1a73bb67\n",
        "# If a fresh model is instantiated, it would need to load saved weights\n",
        "# For the purpose of this execution, we assume the model in memory is the trained one.\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# Disable gradient calculations during evaluation\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0) # Multiply by batch size to get total loss for batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 2.3133\n",
            "Test Accuracy: 9.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82d9601"
      },
      "source": [
        "## Evaluate PyTorch Model (Retry)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained PyTorch CNN model on the MNIST test dataset to calculate its final accuracy and loss, ensuring the correct trained model is used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85c9a2b2",
        "outputId": "12c16c9d-38d1-4547-ecda-c3419a8c6001"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Assuming the 'model' object from the previous training step (1a73bb67) is already in memory and trained.\n",
        "# Do NOT re-instantiate the model here, as it would lose its trained weights.\n",
        "\n",
        "# Define transformations (reusing from previous cells for clarity and self-containment)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST test dataset (reusing from previous cells)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader (reusing from previous cells)\n",
        "batch_size = 64\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Loss function (reusing from previous cells)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Check if GPU is available, otherwise use CPU (reusing from previous cells)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Move the trained model to the correct device\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "# Disable gradient calculations during evaluation\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0) # Multiply by batch size to get total loss for batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 2.3133\n",
            "Test Accuracy: 9.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffab009a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous evaluation step (cell `85c9a2b2`) still showed a low accuracy (9.88%), indicating that the model being evaluated was not the correctly trained one, despite efforts to preserve its state. To guarantee that the evaluation is performed on the *trained* PyTorch CNN model, I will consolidate the entire process—model definition, data loading, training, and evaluation—into a single code block. This ensures that the `model` object is trained and then immediately evaluated without any loss of weights or re-instantiation issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1ca4108",
        "outputId": "2e7ca87d-7feb-44ed-cc27-c641971f0f41"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 1. Define the CNN model architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN()\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "# Check if GPU is available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Training model on {device}\")\n",
        "\n",
        "# 2. Training Loop (re-run from previous steps)\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "print(\"PyTorch model training completed.\")\n",
        "\n",
        "# 3. Evaluation Loop (as per instructions)\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0) # Multiply by batch size to get total loss for batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 13.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 459kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on cuda\n",
            "Epoch [1/5], Loss: 0.1284\n",
            "Epoch [2/5], Loss: 0.0415\n",
            "Epoch [3/5], Loss: 0.0276\n",
            "Epoch [4/5], Loss: 0.0197\n",
            "Epoch [5/5], Loss: 0.0158\n",
            "PyTorch model training completed.\n",
            "\n",
            "Test Loss: 0.0271\n",
            "Test Accuracy: 99.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit."
      ],
      "metadata": {
        "id": "DxxKvfm02ta3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uUqbTdiJHYsV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a361a8"
      },
      "source": [
        "# Task\n",
        "Build and deploy an end-to-end CNN model to classify chest X-ray images as \"Normal\" or \"Pneumonia\" for a medical imaging startup. This includes data preparation, model training, evaluation, and deploying the model as a web application using Streamlit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "635a7532"
      },
      "source": [
        "## Data Collection and Organization\n",
        "\n",
        "\n",
        "Gather a comprehensive dataset of chest X-ray images, categorized into 'Normal' and 'Pneumonia'. Ensure proper licensing and ethical considerations. Organize the dataset into a clear directory structure (e.g., `train/normal`, `train/pneumonia`, `val/normal`, `val/pneumonia`, `test/normal`, `test/pneumonia`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c2f879b"
      },
      "source": [
        "**Reasoning**:\n",
        "To simulate the dataset structure for the chest X-ray classification task, I will create the necessary directory structure for training, validation, and testing sets, each with 'normal' and 'pneumonia' subdirectories. I will also populate these directories with dummy image files using PIL to ensure they are valid for later processing steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c167ed1",
        "outputId": "6c5ce12f-90cd-40fa-a257-a7fd5324dad2"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import shutil\n",
        "\n",
        "# Define base directory and class names\n",
        "base_dataset_dir = 'chest_xray_dataset'\n",
        "class_names = ['normal', 'pneumonia']\n",
        "\n",
        "# Define image dimensions for dummy images\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "\n",
        "# Define the number of dummy images for each split and class\n",
        "dummy_counts = {\n",
        "    'train': {'normal': 20, 'pneumonia': 20},\n",
        "    'val': {'normal': 5, 'pneumonia': 5},\n",
        "    'test': {'normal': 10, 'pneumonia': 10}\n",
        "}\n",
        "\n",
        "# Clean up any existing directory to ensure a fresh start\n",
        "if os.path.exists(base_dataset_dir):\n",
        "    shutil.rmtree(base_dataset_dir)\n",
        "    print(f\"Removed existing directory: {base_dataset_dir}\")\n",
        "\n",
        "# Create the base directory\n",
        "os.makedirs(base_dataset_dir, exist_ok=True)\n",
        "print(f\"Created base directory: {base_dataset_dir}\")\n",
        "\n",
        "# Create train, val, test directories and their class subdirectories\n",
        "for split in dummy_counts.keys():\n",
        "    split_dir = os.path.join(base_dataset_dir, split)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "    print(f\"Created split directory: {split_dir}\")\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(split_dir, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        print(f\"Created class directory: {class_dir}\")\n",
        "\n",
        "        # Populate with dummy images\n",
        "        num_images = dummy_counts[split][class_name]\n",
        "        for i in range(num_images):\n",
        "            dummy_file_path = os.path.join(class_dir, f'{class_name}_{i+1:03d}.jpg')\n",
        "            # Create a small, valid black image using Pillow\n",
        "            img = Image.new('RGB', (IMG_WIDTH, IMG_HEIGHT), color='black')\n",
        "            img.save(dummy_file_path)\n",
        "\n",
        "print(\"Dummy chest X-ray dataset directory structure and files created successfully.\")\n",
        "\n",
        "# Verify the structure\n",
        "print(\"\\nVerifying directory structure:\")\n",
        "for root, dirs, files in os.walk(base_dataset_dir):\n",
        "    level = root.replace(base_dataset_dir, '').count(os.sep)\n",
        "    indent = ' ' * 4 * (level)\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files:\n",
        "        print(f'{subindent}{f}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created base directory: chest_xray_dataset\n",
            "Created split directory: chest_xray_dataset/train\n",
            "Created class directory: chest_xray_dataset/train/normal\n",
            "Created class directory: chest_xray_dataset/train/pneumonia\n",
            "Created split directory: chest_xray_dataset/val\n",
            "Created class directory: chest_xray_dataset/val/normal\n",
            "Created class directory: chest_xray_dataset/val/pneumonia\n",
            "Created split directory: chest_xray_dataset/test\n",
            "Created class directory: chest_xray_dataset/test/normal\n",
            "Created class directory: chest_xray_dataset/test/pneumonia\n",
            "Dummy chest X-ray dataset directory structure and files created successfully.\n",
            "\n",
            "Verifying directory structure:\n",
            "chest_xray_dataset/\n",
            "    val/\n",
            "        normal/\n",
            "            normal_004.jpg\n",
            "            normal_003.jpg\n",
            "            normal_002.jpg\n",
            "            normal_005.jpg\n",
            "            normal_001.jpg\n",
            "        pneumonia/\n",
            "            pneumonia_001.jpg\n",
            "            pneumonia_005.jpg\n",
            "            pneumonia_004.jpg\n",
            "            pneumonia_003.jpg\n",
            "            pneumonia_002.jpg\n",
            "    train/\n",
            "        normal/\n",
            "            normal_009.jpg\n",
            "            normal_004.jpg\n",
            "            normal_019.jpg\n",
            "            normal_012.jpg\n",
            "            normal_003.jpg\n",
            "            normal_020.jpg\n",
            "            normal_016.jpg\n",
            "            normal_002.jpg\n",
            "            normal_008.jpg\n",
            "            normal_006.jpg\n",
            "            normal_014.jpg\n",
            "            normal_010.jpg\n",
            "            normal_005.jpg\n",
            "            normal_011.jpg\n",
            "            normal_017.jpg\n",
            "            normal_007.jpg\n",
            "            normal_013.jpg\n",
            "            normal_001.jpg\n",
            "            normal_018.jpg\n",
            "            normal_015.jpg\n",
            "        pneumonia/\n",
            "            pneumonia_017.jpg\n",
            "            pneumonia_001.jpg\n",
            "            pneumonia_005.jpg\n",
            "            pneumonia_016.jpg\n",
            "            pneumonia_009.jpg\n",
            "            pneumonia_014.jpg\n",
            "            pneumonia_011.jpg\n",
            "            pneumonia_013.jpg\n",
            "            pneumonia_006.jpg\n",
            "            pneumonia_004.jpg\n",
            "            pneumonia_003.jpg\n",
            "            pneumonia_002.jpg\n",
            "            pneumonia_020.jpg\n",
            "            pneumonia_007.jpg\n",
            "            pneumonia_012.jpg\n",
            "            pneumonia_015.jpg\n",
            "            pneumonia_008.jpg\n",
            "            pneumonia_010.jpg\n",
            "            pneumonia_018.jpg\n",
            "            pneumonia_019.jpg\n",
            "    test/\n",
            "        normal/\n",
            "            normal_009.jpg\n",
            "            normal_004.jpg\n",
            "            normal_003.jpg\n",
            "            normal_002.jpg\n",
            "            normal_008.jpg\n",
            "            normal_006.jpg\n",
            "            normal_010.jpg\n",
            "            normal_005.jpg\n",
            "            normal_007.jpg\n",
            "            normal_001.jpg\n",
            "        pneumonia/\n",
            "            pneumonia_001.jpg\n",
            "            pneumonia_005.jpg\n",
            "            pneumonia_009.jpg\n",
            "            pneumonia_006.jpg\n",
            "            pneumonia_004.jpg\n",
            "            pneumonia_003.jpg\n",
            "            pneumonia_002.jpg\n",
            "            pneumonia_007.jpg\n",
            "            pneumonia_008.jpg\n",
            "            pneumonia_010.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fe72a3b"
      },
      "source": [
        "## Data Preprocessing and Augmentation\n",
        "\n",
        "### Subtask:\n",
        "Implement data preprocessing steps such as resizing images to a consistent dimension, normalizing pixel values, and applying data augmentation techniques (e.g., rotation, shifting, zooming, flipping) to increase the diversity of the training data and reduce overfitting. Use Keras `ImageDataGenerator` or PyTorch `transforms` for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85cc1eb5"
      },
      "source": [
        "**Reasoning**:\n",
        "I will now implement the data preprocessing and augmentation steps using Keras `ImageDataGenerator` as outlined in the instructions. This involves defining image dimensions, setting up `ImageDataGenerator` instances for training (with augmentation) and validation/test (without augmentation), and creating `flow_from_directory` generators for each dataset split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1299c4e",
        "outputId": "1a8a68f0-6ed2-42ac-cf71-3abfe107aeaf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Image dimensions defined in previous steps\n",
        "# IMG_WIDTH = 128\n",
        "# IMG_HEIGHT = 128\n",
        "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3) # RGB images\n",
        "\n",
        "# Define paths for dataset splits\n",
        "train_dir = os.path.join(base_dataset_dir, 'train')\n",
        "validation_dir = os.path.join(base_dataset_dir, 'val')\n",
        "test_dir = os.path.join(base_dataset_dir, 'test')\n",
        "\n",
        "# Instantiate ImageDataGenerator for training with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Instantiate ImageDataGenerator for validation and test (only rescaling)\n",
        "validation_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create train_generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create validation_generator\n",
        "validation_generator = validation_test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create test_generator\n",
        "test_generator = validation_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False # Keep data in order for evaluation\n",
        ")\n",
        "\n",
        "print(\"Data generators for training, validation, and testing created successfully.\")\n",
        "print(f\"Train samples: {train_generator.samples}, Classes: {train_generator.class_indices}\")\n",
        "print(f\"Validation samples: {validation_generator.samples}, Classes: {validation_generator.class_indices}\")\n",
        "print(f\"Test samples: {test_generator.samples}, Classes: {test_generator.class_indices}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 40 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n",
            "Data generators for training, validation, and testing created successfully.\n",
            "Train samples: 40, Classes: {'normal': 0, 'pneumonia': 1}\n",
            "Validation samples: 10, Classes: {'normal': 0, 'pneumonia': 1}\n",
            "Test samples: 20, Classes: {'normal': 0, 'pneumonia': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2317d15b"
      },
      "source": [
        "## CNN Model Architecture Definition\n",
        "\n",
        "### Subtask:\n",
        "Design or choose an appropriate CNN architecture for image classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9c2eded"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the CNN model architecture using `tf.keras.Sequential` as specified in the instructions, including `Conv2D`, `MaxPooling2D`, `Flatten`, and `Dense` layers, with the correct input shape and output layer for binary classification. Then, I will print the model summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "b95b7555",
        "outputId": "1e8671b5-fab4-4949-d4e9-28d87b133ad6"
      },
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=IMG_SHAPE),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(len(class_names), activation='softmax') # Number of classes (2 for normal/pneumonia)\n",
        "])\n",
        "\n",
        "print(\"CNN model architecture defined successfully.\")\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model architecture defined successfully.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m12,845,568\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m1,026\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,568</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,939,842\u001b[0m (49.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,939,842</span> (49.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,939,842\u001b[0m (49.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,939,842</span> (49.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f933d132"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the CNN model architecture is defined, the next logical step is to compile it using an appropriate optimizer, loss function, and metrics for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c525e15f",
        "outputId": "f677320e-87bd-4a95-c9f5-f2b8e4be0b25"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"CNN model compiled successfully.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model compiled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32f2e070"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the CNN model is compiled, the next step is to train it using the prepared data generators for training and validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fed445c",
        "outputId": "894ca4e2-e1b9-4b31-ab49-1f37c77e7da0"
      },
      "source": [
        "epochs = 10\n",
        "\n",
        "# Calculate steps per epoch for train and validation generators\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.6250 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.4688 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.3750 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.5312 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8750 - loss: 0.6922 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.4062 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.3750 - loss: 0.6936 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.5312 - loss: 0.6930 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.2500 - loss: 0.6942 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.5625 - loss: 0.6929 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "CNN model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66d497fe"
      },
      "source": [
        "**Reasoning**:\n",
        "With the model trained, the next critical step is to evaluate its performance on the unseen test dataset to assess its generalization capabilities. I will use `model.evaluate()` with the `test_generator` to get the final test loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f603ddb4",
        "outputId": "976be005-5a23-444f-a29c-3ee8522d6d8c"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 970ms/step - accuracy: 0.5000 - loss: 0.6931\n",
            "Test Loss: 0.6931\n",
            "Test Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84ed940"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous training and evaluation showed 50% accuracy, indicating the model didn't learn, likely due to a `batch_size` too large for the small dummy dataset, resulting in `steps_per_epoch` and `validation_steps` being too low or zero. I will regenerate the data generators with a `batch_size` of 1 to ensure meaningful steps for training and evaluation, and then re-execute the evaluation step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1786615e",
        "outputId": "6c3c52af-1617-4cac-8b02-040b5b90a814"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Image dimensions defined in previous steps\n",
        "# IMG_WIDTH = 128\n",
        "# IMG_HEIGHT = 128\n",
        "IMG_SHAPE = (IMG_WIDTH, IMG_HEIGHT, 3) # RGB images\n",
        "\n",
        "# Define paths for dataset splits\n",
        "train_dir = os.path.join(base_dataset_dir, 'train')\n",
        "validation_dir = os.path.join(base_dataset_dir, 'val')\n",
        "test_dir = os.path.join(base_dataset_dir, 'test')\n",
        "\n",
        "# Instantiate ImageDataGenerator for training with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Instantiate ImageDataGenerator for validation and test (only rescaling)\n",
        "validation_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "batch_size = 1 # Set batch_size to 1 for the small dummy dataset to ensure enough steps\n",
        "\n",
        "# Create train_generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create validation_generator\n",
        "validation_generator = validation_test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Create test_generator\n",
        "test_generator = validation_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False # Keep data in order for evaluation\n",
        ")\n",
        "\n",
        "print(\"Data generators for training, validation, and testing created successfully.\")\n",
        "print(f\"Train samples: {train_generator.samples}, Classes: {train_generator.class_indices}\")\n",
        "print(f\"Validation samples: {validation_generator.samples}, Classes: {validation_generator.class_indices}\")\n",
        "print(f\"Test samples: {test_generator.samples}, Classes: {test_generator.class_indices}\")\n",
        "\n",
        "# Re-define model (since batch_size affects subsequent training)\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=IMG_SHAPE),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(len(class_names), activation='softmax') # Number of classes (2 for normal/pneumonia)\n",
        "])\n",
        "\n",
        "print(\"CNN model architecture re-defined successfully.\")\n",
        "model.summary()\n",
        "\n",
        "# Re-compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"CNN model re-compiled successfully.\")\n",
        "\n",
        "# Re-train model\n",
        "epochs = 10\n",
        "\n",
        "# Calculate steps per epoch for train and validation generators\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps\n",
        ")\n",
        "\n",
        "print(\"CNN model training completed.\")\n",
        "\n",
        "# Re-evaluate model\n",
        "loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // batch_size)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 40 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n",
            "Data generators for training, validation, and testing created successfully.\n",
            "Train samples: 40, Classes: {'normal': 0, 'pneumonia': 1}\n",
            "Validation samples: 10, Classes: {'normal': 0, 'pneumonia': 1}\n",
            "Test samples: 20, Classes: {'normal': 0, 'pneumonia': 1}\n",
            "CNN model architecture re-defined successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m12,845,568\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m1,026\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,845,568</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,939,842\u001b[0m (49.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,939,842</span> (49.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,939,842\u001b[0m (49.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,939,842</span> (49.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model re-compiled successfully.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.4360 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3595 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3383 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3638 - loss: 0.6933 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5428 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5175 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.5673 - loss: 0.6932 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.3351 - loss: 0.6937 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6303 - loss: 0.6928 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6186 - loss: 0.6923 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "CNN model training completed.\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8185 - loss: 0.6921\n",
            "Test Loss: 0.6931\n",
            "Test Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b6daad"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution still shows 50% accuracy, which is expected for a binary classification model trained on non-discriminative (all black) dummy images, even with a `batch_size` of 1. Since the purpose of this section is to demonstrate the end-to-end process, the next logical step after training and evaluating (even on dummy data) is to save the trained model for future deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc459d49",
        "outputId": "d2700311-7891-4cbc-f692-49deaf6db780"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define a path to save the model\n",
        "model_save_path = 'chest_xray_cnn_model.keras'\n",
        "\n",
        "# Save the model\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"Model saved successfully to {model_save_path}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully to chest_xray_cnn_model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84e98284"
      },
      "source": [
        "## Streamlit Web Application Development\n",
        "\n",
        "### Subtask:\n",
        "Develop a Streamlit application (`app.py`) that allows users to upload a chest X-ray image, preprocesses it, loads the trained CNN model, makes a prediction, and displays the result with a user-friendly interface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b682ce5"
      },
      "source": [
        "```python\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# --- Configuration --- #\n",
        "MODEL_PATH = 'chest_xray_cnn_model.keras'\n",
        "IMAGE_SIZE = (128, 128)\n",
        "CLASS_NAMES = ['Normal', 'Pneumonia']\n",
        "\n",
        "# --- Load the pre-trained model --- #\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        st.error(f\"Model file not found at {MODEL_PATH}. Please ensure the model is saved correctly.\")\n",
        "        return None\n",
        "    try:\n",
        "        model = tf.keras.models.load_model(MODEL_PATH)\n",
        "        st.success(\"Model loaded successfully!\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading the model: {e}\")\n",
        "        return None\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# --- Streamlit App Layout --- #\n",
        "st.set_page_config(page_title=\"Chest X-ray Classifier\", layout=\"centered\")\n",
        "\n",
        "st.title(\"🩺 Chest X-ray Pneumonia Classifier\")\n",
        "st.markdown(\"Upload a chest X-ray image (JPG, JPEG, PNG) to classify it as Normal or Pneumonia.\")\n",
        "\n",
        "# --- File Uploader --- #\n",
        "uploaded_file = st.file_uploader(\n",
        "    \"Choose an image...\",\n",
        "    type=[\"jpg\", \"jpeg\", \"png\"],\n",
        "    help=\"Upload an image file for classification.\"\n",
        ")\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    if model is None:\n",
        "        st.warning(\"Model could not be loaded. Please check the model path and file.\")\n",
        "    else:\n",
        "        # Display the uploaded image\n",
        "        image = Image.open(uploaded_file).convert('RGB') # Ensure image is RGB\n",
        "        st.image(image, caption='Uploaded X-ray Image', use_column_width=True)\n",
        "        st.write(\"\")\n",
        "        st.write(\"Classifying...\")\n",
        "\n",
        "        # Preprocess the image\n",
        "        # Resize\n",
        "        image = image.resize(IMAGE_SIZE)\n",
        "        # Convert to numpy array and normalize\n",
        "        img_array = np.asarray(image) / 255.0\n",
        "        # Expand dimensions to match model input shape (1, height, width, channels)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Make prediction\n",
        "        predictions = model.predict(img_array)\n",
        "        predicted_class_idx = np.argmax(predictions, axis=1)[0]\n",
        "        confidence = predictions[0][predicted_class_idx] * 100\n",
        "\n",
        "        predicted_class_name = CLASS_NAMES[predicted_class_idx]\n",
        "\n",
        "        # Display result\n",
        "        st.success(f\"Prediction: **{predicted_class_name}**\")\n",
        "        st.info(f\"Confidence: **{confidence:.2f}%**\")\n",
        "\n",
        "        if predicted_class_name == 'Pneumonia':\n",
        "            st.warning(\"**Disclaimer:** This is an AI-based prediction and should not be used as a substitute for professional medical advice. Always consult with a qualified healthcare professional for diagnosis and treatment.\")\n",
        "        else:\n",
        "            st.success(\"**Disclaimer:** This is an AI-based prediction. Always consult with a qualified healthcare professional for diagnosis and treatment.\")\n",
        "\n",
        "# --- Instructions to run the app --- #\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    ---\n",
        "    **To run this application:**\n",
        "    1. Save the code above into a file named `app.py` in the same directory as your `chest_xray_cnn_model.keras` file.\n",
        "    2. Open your terminal or command prompt.\n",
        "    3. Navigate to the directory where you saved `app.py`.\n",
        "    4. Run the command: `streamlit run app.py`\n",
        "    5. The application will open in your web browser.\n",
        "    \"\"\"\n",
        ")\n",
        "```"
      ]
    }
  ]
}